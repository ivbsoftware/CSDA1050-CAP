{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  GTA News 50 Clusters Chart Using Packed Circles Rendering v.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load 50 cluster dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# directories\n",
    "dir_path = os.getcwd()\n",
    "#print('Working dir: ' + dir_path)\n",
    "\n",
    "local_path = dir_path + '\\\\..\\\\gta-news\\\\doc2vec\\data\\\\'\n",
    "df = pd.read_pickle(local_path+'backup'+'-gta.50'+'.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>cluster</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-04-20</td>\n",
       "      <td>\"this is why we can't have nice things in nyc,...</td>\n",
       "      <td>http://dagblog.com/reader-blogs/why-we-cant-ha...</td>\n",
       "      <td>13</td>\n",
       "      <td>[-0.47769657, 0.3672163, 0.23536347, 0.5756423...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-04-20</td>\n",
       "      <td>rural book borrowing in peril as libraries sla...</td>\n",
       "      <td>http://easternontarionetwork.com/2019/04/20/ru...</td>\n",
       "      <td>13</td>\n",
       "      <td>[-0.5722261, -0.26479113, -0.1152498, 0.664171...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-04-20</td>\n",
       "      <td>\"discussing canada's new us-focused cannabis e...</td>\n",
       "      <td>http://www.benzinga.com/markets/cannabis/19/04...</td>\n",
       "      <td>11</td>\n",
       "      <td>[0.54588157, -0.26139393, -0.18809983, -0.0384...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-04-20</td>\n",
       "      <td>serea restaurant slated to open in hotel del c...</td>\n",
       "      <td>http://www.coronadonewsca.com/news/coronado_ho...</td>\n",
       "      <td>25</td>\n",
       "      <td>[-0.28711024, -0.2106441, -0.61289483, 0.04533...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-04-20</td>\n",
       "      <td>federal trial of vernon man accused of abducti...</td>\n",
       "      <td>http://www.courant.com/news/connecticut/hc-new...</td>\n",
       "      <td>46</td>\n",
       "      <td>[0.062113207, 0.0157832, 0.23044105, 0.4720517...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date                                              title  \\\n",
       "0 2019-04-20  \"this is why we can't have nice things in nyc,...   \n",
       "1 2019-04-20  rural book borrowing in peril as libraries sla...   \n",
       "2 2019-04-20  \"discussing canada's new us-focused cannabis e...   \n",
       "3 2019-04-20  serea restaurant slated to open in hotel del c...   \n",
       "4 2019-04-20  federal trial of vernon man accused of abducti...   \n",
       "\n",
       "                                                 url  cluster  \\\n",
       "0  http://dagblog.com/reader-blogs/why-we-cant-ha...       13   \n",
       "1  http://easternontarionetwork.com/2019/04/20/ru...       13   \n",
       "2  http://www.benzinga.com/markets/cannabis/19/04...       11   \n",
       "3  http://www.coronadonewsca.com/news/coronado_ho...       25   \n",
       "4  http://www.courant.com/news/connecticut/hc-new...       46   \n",
       "\n",
       "                                              vector  \n",
       "0  [-0.47769657, 0.3672163, 0.23536347, 0.5756423...  \n",
       "1  [-0.5722261, -0.26479113, -0.1152498, 0.664171...  \n",
       "2  [0.54588157, -0.26139393, -0.18809983, -0.0384...  \n",
       "3  [-0.28711024, -0.2106441, -0.61289483, 0.04533...  \n",
       "4  [0.062113207, 0.0157832, 0.23044105, 0.4720517...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get cluster word counts from titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ibaranov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ibaranov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../gta-news/doc2vec'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import d2v_utils\n",
    "skip_terms =['toronto','canada','canadian','ontario']\n",
    "cluster_descr = []\n",
    "clusters = df.groupby(['cluster'])['title']\n",
    "for cluster, titles in clusters:\n",
    "    #print(\"\\nCluster: \", cluster)\n",
    "    filtered_words = []\n",
    "    for title in titles:\n",
    "        t = title[0:-4]\n",
    "        #print(\">>>\", t)\n",
    "        tokens = d2v_utils.prepare_text_for_lda(t)\n",
    "        tokens = [word for word in tokens if word not in skip_terms and not word.isdigit()]\n",
    "        #print(\"  >\", tokens)\n",
    "        filtered_words = filtered_words + tokens\n",
    "    count = Counter(filtered_words)\n",
    "    current_clust_descr = count.most_common()[:10] \n",
    "    cluster_descr.append(current_clust_descr)\n",
    "\n",
    "clust_num = len(cluster_descr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('announce', 195), ('result', 47), ('cannabis', 44), ('project', 43), ('update', 43)]\n",
      "[('refugee', 54), ('asylum', 47), ('snowden', 46), ('shelter', 45), ('grant', 30)]\n",
      "[('weather', 22), ('winter', 21), ('storm', 16), ('school', 11), ('sweep', 9)]\n",
      "[('cannabis', 69), ('store', 26), ('legal', 17), ('medical', 14), ('illegal', 14)]\n",
      "[('woman', 21), ('survivor', 18), ('family', 16), ('holocaust', 13), ('years', 13)]\n",
      "[('research', 19), ('university', 18), ('innovation', 14), ('better', 13), ('company', 13)]\n"
     ]
    }
   ],
   "source": [
    "print(cluster_descr[0][0:5])\n",
    "print(cluster_descr[1][0:5])\n",
    "print(cluster_descr[2][0:5])\n",
    "print(cluster_descr[3][0:5])\n",
    "print(cluster_descr[4][0:5])\n",
    "print(cluster_descr[5][0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load cluster coordinates generated by circle packing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>date</th>\n",
       "      <th>cluster</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>radius</th>\n",
       "      <th>transition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21</td>\n",
       "      <td>2019-01-21</td>\n",
       "      <td>0</td>\n",
       "      <td>950.64150</td>\n",
       "      <td>495.21707</td>\n",
       "      <td>162.16217</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>2019-01-21</td>\n",
       "      <td>14</td>\n",
       "      <td>1036.73280</td>\n",
       "      <td>639.83203</td>\n",
       "      <td>162.16217</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>2019-01-21</td>\n",
       "      <td>42</td>\n",
       "      <td>1118.33090</td>\n",
       "      <td>493.97827</td>\n",
       "      <td>162.16217</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>2019-01-21</td>\n",
       "      <td>18</td>\n",
       "      <td>792.79425</td>\n",
       "      <td>362.96730</td>\n",
       "      <td>146.62161</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>2019-01-21</td>\n",
       "      <td>20</td>\n",
       "      <td>640.21120</td>\n",
       "      <td>498.38596</td>\n",
       "      <td>146.62161</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   day        date  cluster           x          y     radius  transition\n",
       "0   21  2019-01-21        0   950.64150  495.21707  162.16217           0\n",
       "1   21  2019-01-21       14  1036.73280  639.83203  162.16217           0\n",
       "2   21  2019-01-21       42  1118.33090  493.97827  162.16217           0\n",
       "3   21  2019-01-21       18   792.79425  362.96730  146.62161           0\n",
       "4   21  2019-01-21       20   640.21120  498.38596  146.62161           0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_path = dir_path + '\\\\..\\\\dev\\\\packing\\\\circle_pack\\\\'\n",
    "dfCPack = pd.read_csv(local_path+'frameSequence.3.csv')\n",
    "dfCPack.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>date</th>\n",
       "      <th>cluster</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>radius</th>\n",
       "      <th>transition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>21</td>\n",
       "      <td>2019-01-21</td>\n",
       "      <td>4</td>\n",
       "      <td>869.0561</td>\n",
       "      <td>856.90643</td>\n",
       "      <td>107.77027</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>21</td>\n",
       "      <td>2019-01-21</td>\n",
       "      <td>16</td>\n",
       "      <td>1038.5347</td>\n",
       "      <td>832.35100</td>\n",
       "      <td>107.77027</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>21</td>\n",
       "      <td>2019-01-21</td>\n",
       "      <td>17</td>\n",
       "      <td>1446.1097</td>\n",
       "      <td>654.71380</td>\n",
       "      <td>107.77027</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>21</td>\n",
       "      <td>2019-01-21</td>\n",
       "      <td>19</td>\n",
       "      <td>1484.6389</td>\n",
       "      <td>436.51022</td>\n",
       "      <td>107.77027</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>21</td>\n",
       "      <td>2019-01-21</td>\n",
       "      <td>22</td>\n",
       "      <td>1385.3837</td>\n",
       "      <td>314.13232</td>\n",
       "      <td>107.77027</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    day        date  cluster          x          y     radius  transition\n",
       "37   21  2019-01-21        4   869.0561  856.90643  107.77027           0\n",
       "38   21  2019-01-21       16  1038.5347  832.35100  107.77027           0\n",
       "39   21  2019-01-21       17  1446.1097  654.71380  107.77027           0\n",
       "40   21  2019-01-21       19  1484.6389  436.51022  107.77027           0\n",
       "41   21  2019-01-21       22  1385.3837  314.13232  107.77027           0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfChart = dfCPack.sort_values(['date','transition','radius'])\n",
    "dfChart.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines wordcloud circular mask positioned on image\n",
    "def create_circular_mask(w, h, center=None, radius=None):\n",
    "    if center is None: # use the middle of the image\n",
    "        center = [int(w/2), int(h/2)]\n",
    "    if radius is None: # use the smallest distance between the center and image walls\n",
    "        radius = min(center[0], center[1], w-center[0], h-center[1])\n",
    "    y, x = np.ogrid[:h, :w]\n",
    "    mask = (y - center[1]) ** 2 + (x - center[0]) ** 2 > radius ** 2\n",
    "    mask = 255 * mask.astype(int)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(152, 0, 158)\n",
      "(3, 160, 154)\n",
      "(151, 153, 4)\n",
      "(169, 122, 7)\n"
     ]
    }
   ],
   "source": [
    "import colorsys\n",
    "import random\n",
    "random.seed(111)\n",
    " \n",
    "def get_colors(n):\n",
    "  ret = []\n",
    "  for i in range(n):\n",
    "    hue = random.random()\n",
    "    lightness  = 0.3 + random.random() * 0.05\n",
    "    saturation = 0.9 + random.random() * 0.1\n",
    "    rgb = colorsys.hls_to_rgb(hue, lightness, saturation)\n",
    "    ret.append((int(rgb[0] * 256),int(rgb[1] * 256),int(rgb[2] * 256))) \n",
    "  return ret\n",
    "\n",
    "colors = get_colors(clust_num)\n",
    "\n",
    "print(colors[0])\n",
    "print(colors[1])\n",
    "print(colors[2])\n",
    "print(colors[30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Render wordclouds of biggest sizes separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max radius for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>radius</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>239.86487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>302.02704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>193.24324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>154.39189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>154.39189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster     radius\n",
       "0        0  239.86487\n",
       "1        1  302.02704\n",
       "2        2  193.24324\n",
       "3        3  154.39189\n",
       "4        4  154.39189"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfMaxRadius = dfCPack[dfCPack.transition == 0][['cluster','radius']]\\\n",
    "    .groupby(['cluster']).agg('max').reset_index()\n",
    "dfMaxRadius.cluster = dfMaxRadius.cluster.astype(int)\n",
    "dfMaxRadius.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Array max images for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...preparing image for cluster:  0 , diameter:  240.0\n",
      "...preparing image for cluster:  1 , diameter:  303.0\n",
      "...preparing image for cluster:  2 , diameter:  194.0\n",
      "...preparing image for cluster:  3 , diameter:  155.0\n",
      "...preparing image for cluster:  4 , diameter:  155.0\n",
      "...preparing image for cluster:  5 , diameter:  194.0\n",
      "...preparing image for cluster:  6 , diameter:  334.0\n",
      "...preparing image for cluster:  7 , diameter:  186.0\n",
      "...preparing image for cluster:  8 , diameter:  287.0\n",
      "...preparing image for cluster:  9 , diameter:  217.0\n",
      "...preparing image for cluster:  10 , diameter:  217.0\n",
      "...preparing image for cluster:  11 , diameter:  240.0\n",
      "...preparing image for cluster:  12 , diameter:  202.0\n",
      "...preparing image for cluster:  13 , diameter:  256.0\n",
      "...preparing image for cluster:  14 , diameter:  256.0\n",
      "...preparing image for cluster:  15 , diameter:  225.0\n",
      "...preparing image for cluster:  16 , diameter:  225.0\n",
      "...preparing image for cluster:  17 , diameter:  202.0\n",
      "...preparing image for cluster:  18 , diameter:  194.0\n",
      "...preparing image for cluster:  19 , diameter:  225.0\n",
      "...preparing image for cluster:  20 , diameter:  256.0\n",
      "...preparing image for cluster:  21 , diameter:  178.0\n",
      "...preparing image for cluster:  22 , diameter:  178.0\n",
      "...preparing image for cluster:  23 , diameter:  225.0\n",
      "...preparing image for cluster:  24 , diameter:  194.0\n",
      "...preparing image for cluster:  25 , diameter:  163.0\n",
      "...preparing image for cluster:  26 , diameter:  326.0\n",
      "...preparing image for cluster:  27 , diameter:  194.0\n",
      "...preparing image for cluster:  28 , diameter:  256.0\n",
      "...preparing image for cluster:  29 , diameter:  209.0\n",
      "...preparing image for cluster:  30 , diameter:  217.0\n",
      "...preparing image for cluster:  31 , diameter:  155.0\n",
      "...preparing image for cluster:  32 , diameter:  194.0\n",
      "...preparing image for cluster:  33 , diameter:  318.0\n",
      "...preparing image for cluster:  34 , diameter:  163.0\n",
      "...preparing image for cluster:  35 , diameter:  194.0\n",
      "...preparing image for cluster:  36 , diameter:  248.0\n",
      "...preparing image for cluster:  37 , diameter:  163.0\n",
      "...preparing image for cluster:  38 , diameter:  155.0\n",
      "...preparing image for cluster:  39 , diameter:  155.0\n",
      "...preparing image for cluster:  40 , diameter:  170.0\n",
      "...preparing image for cluster:  41 , diameter:  217.0\n",
      "...preparing image for cluster:  42 , diameter:  233.0\n",
      "...preparing image for cluster:  43 , diameter:  644.0\n",
      "...preparing image for cluster:  44 , diameter:  240.0\n",
      "...preparing image for cluster:  45 , diameter:  248.0\n",
      "...preparing image for cluster:  46 , diameter:  380.0\n",
      "...preparing image for cluster:  47 , diameter:  310.0\n",
      "...preparing image for cluster:  48 , diameter:  675.0\n",
      "...preparing image for cluster:  49 , diameter:  248.0\n"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Screen size: 4K 3840 x 2160 ?\n",
    "img_pix_x = 3840\n",
    "img_pix_y = 2160\n",
    "\n",
    "pack_img_pix_x = 1778\n",
    "pack_img_pix_y = 1000\n",
    "\n",
    "x_scale = 1.0 #* img_pix_x / pack_img_pix_x\n",
    "y_scale = 1.0 #* img_pix_y / pack_img_pix_y\n",
    "\n",
    "images = []\n",
    "\n",
    "scale = x_scale\n",
    "for index, row in dfMaxRadius.iterrows():\n",
    "    \n",
    "    cluster = int(row.cluster)\n",
    "    radius = math.ceil(row.radius) * scale\n",
    "    print(\"...preparing image for cluster: \", cluster, \", diameter: \", radius)\n",
    "\n",
    "    # make the cloud\n",
    "    mask = create_circular_mask(radius, radius)\n",
    "    wc = WordCloud(background_color=\"white\", random_state=33, \n",
    "                   mask=mask,\n",
    "                   color_func=lambda *args, **kwargs: colors[cluster])\n",
    "\n",
    "    wc.generate_from_frequencies(dict(cluster_descr[cluster]))\n",
    "\n",
    "    # make white pix transparent\n",
    "    img = wc.to_image().convert('RGBA')\n",
    "    datas = img.getdata()\n",
    "    newData = []\n",
    "    for item in datas:\n",
    "        if item[0] == 255 and item[1] == 255 and item[2] == 255:\n",
    "            newData.append((255, 255, 255, 0))\n",
    "        else:\n",
    "            newData.append(item)\n",
    "    img.putdata(newData)\n",
    "    \n",
    "    #store\n",
    "    img.save(\"wc_circles\\circle.{0}.png\".format(index), \"PNG\")\n",
    "    images.append(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "plt.imshow(images[49])\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render Frames and Transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-21 , transition  0\n",
      "2019-01-21 , transition  2\n",
      "2019-01-21 , transition  4\n",
      "2019-01-21 , transition  6\n",
      "2019-01-21 , transition  8\n",
      "2019-01-22 , transition  0\n",
      "2019-01-22 , transition  2\n",
      "2019-01-22 , transition  4\n",
      "2019-01-22 , transition  6\n",
      "2019-01-22 , transition  8\n",
      "2019-01-23 , transition  0\n",
      "2019-01-23 , transition  2\n",
      "2019-01-23 , transition  4\n",
      "2019-01-23 , transition  6\n",
      "2019-01-23 , transition  8\n",
      "2019-01-24 , transition  0\n",
      "2019-01-24 , transition  2\n",
      "2019-01-24 , transition  4\n",
      "2019-01-24 , transition  6\n",
      "2019-01-24 , transition  8\n",
      "2019-01-25 , transition  0\n",
      "2019-01-25 , transition  2\n",
      "2019-01-25 , transition  4\n",
      "2019-01-25 , transition  6\n",
      "2019-01-25 , transition  8\n",
      "2019-01-26 , transition  0\n",
      "2019-01-26 , transition  2\n",
      "2019-01-26 , transition  4\n",
      "2019-01-26 , transition  6\n",
      "2019-01-26 , transition  8\n",
      "2019-01-27 , transition  0\n",
      "2019-01-27 , transition  2\n",
      "2019-01-27 , transition  4\n",
      "2019-01-27 , transition  6\n",
      "2019-01-27 , transition  8\n",
      "2019-01-28 , transition  0\n",
      "2019-01-28 , transition  2\n",
      "2019-01-28 , transition  4\n",
      "2019-01-28 , transition  6\n",
      "2019-01-28 , transition  8\n",
      "2019-01-29 , transition  0\n",
      "2019-01-29 , transition  2\n",
      "2019-01-29 , transition  4\n",
      "2019-01-29 , transition  6\n",
      "2019-01-29 , transition  8\n",
      "2019-01-30 , transition  0\n",
      "2019-01-30 , transition  2\n",
      "2019-01-30 , transition  4\n",
      "2019-01-30 , transition  6\n",
      "2019-01-30 , transition  8\n",
      "2019-01-31 , transition  0\n",
      "2019-01-31 , transition  2\n",
      "2019-01-31 , transition  4\n",
      "2019-01-31 , transition  6\n",
      "2019-01-31 , transition  8\n",
      "2019-02-01 , transition  0\n",
      "2019-02-01 , transition  2\n",
      "2019-02-01 , transition  4\n",
      "2019-02-01 , transition  6\n",
      "2019-02-01 , transition  8\n",
      "2019-02-02 , transition  0\n",
      "2019-02-02 , transition  2\n",
      "2019-02-02 , transition  4\n",
      "2019-02-02 , transition  6\n",
      "2019-02-02 , transition  8\n",
      "2019-02-03 , transition  0\n",
      "2019-02-03 , transition  2\n",
      "2019-02-03 , transition  4\n",
      "2019-02-03 , transition  6\n",
      "2019-02-03 , transition  8\n",
      "2019-02-04 , transition  0\n",
      "2019-02-04 , transition  2\n",
      "2019-02-04 , transition  4\n",
      "2019-02-04 , transition  6\n",
      "2019-02-04 , transition  8\n",
      "2019-02-05 , transition  0\n",
      "2019-02-05 , transition  2\n",
      "2019-02-05 , transition  4\n",
      "2019-02-05 , transition  6\n",
      "2019-02-05 , transition  8\n",
      "2019-02-06 , transition  0\n",
      "2019-02-06 , transition  2\n",
      "2019-02-06 , transition  4\n",
      "2019-02-06 , transition  6\n",
      "2019-02-06 , transition  8\n",
      "2019-02-07 , transition  0\n",
      "2019-02-07 , transition  2\n",
      "2019-02-07 , transition  4\n",
      "2019-02-07 , transition  6\n",
      "2019-02-07 , transition  8\n",
      "2019-02-08 , transition  0\n",
      "2019-02-08 , transition  2\n",
      "2019-02-08 , transition  4\n",
      "2019-02-08 , transition  6\n",
      "2019-02-08 , transition  8\n",
      "2019-02-09 , transition  0\n",
      "2019-02-09 , transition  2\n",
      "2019-02-09 , transition  4\n",
      "2019-02-09 , transition  6\n",
      "2019-02-09 , transition  8\n",
      "2019-02-10 , transition  0\n",
      "2019-02-10 , transition  2\n",
      "2019-02-10 , transition  4\n",
      "2019-02-10 , transition  6\n",
      "2019-02-10 , transition  8\n",
      "2019-02-11 , transition  0\n",
      "2019-02-11 , transition  2\n",
      "2019-02-11 , transition  4\n",
      "2019-02-11 , transition  6\n",
      "2019-02-11 , transition  8\n",
      "2019-02-12 , transition  0\n",
      "2019-02-12 , transition  2\n",
      "2019-02-12 , transition  4\n",
      "2019-02-12 , transition  6\n",
      "2019-02-12 , transition  8\n",
      "2019-02-13 , transition  0\n",
      "2019-02-13 , transition  2\n",
      "2019-02-13 , transition  4\n",
      "2019-02-13 , transition  6\n",
      "2019-02-13 , transition  8\n",
      "2019-02-14 , transition  0\n",
      "2019-02-14 , transition  2\n",
      "2019-02-14 , transition  4\n",
      "2019-02-14 , transition  6\n",
      "2019-02-14 , transition  8\n",
      "2019-02-15 , transition  0\n",
      "2019-02-15 , transition  2\n",
      "2019-02-15 , transition  4\n",
      "2019-02-15 , transition  6\n",
      "2019-02-15 , transition  8\n",
      "2019-02-16 , transition  0\n",
      "2019-02-16 , transition  2\n",
      "2019-02-16 , transition  4\n",
      "2019-02-16 , transition  6\n",
      "2019-02-16 , transition  8\n",
      "2019-02-17 , transition  0\n",
      "2019-02-17 , transition  2\n",
      "2019-02-17 , transition  4\n",
      "2019-02-17 , transition  6\n",
      "2019-02-17 , transition  8\n",
      "2019-02-18 , transition  0\n",
      "2019-02-18 , transition  2\n",
      "2019-02-18 , transition  4\n",
      "2019-02-18 , transition  6\n",
      "2019-02-18 , transition  8\n",
      "2019-02-19 , transition  0\n",
      "2019-02-19 , transition  2\n",
      "2019-02-19 , transition  4\n",
      "2019-02-19 , transition  6\n",
      "2019-02-19 , transition  8\n",
      "2019-02-20 , transition  0\n",
      "2019-02-20 , transition  2\n",
      "2019-02-20 , transition  4\n",
      "2019-02-20 , transition  6\n",
      "2019-02-20 , transition  8\n",
      "2019-02-21 , transition  0\n",
      "2019-02-21 , transition  2\n",
      "2019-02-21 , transition  4\n",
      "2019-02-21 , transition  6\n",
      "2019-02-21 , transition  8\n",
      "2019-02-22 , transition  0\n",
      "2019-02-22 , transition  2\n",
      "2019-02-22 , transition  4\n",
      "2019-02-22 , transition  6\n",
      "2019-02-22 , transition  8\n",
      "2019-02-23 , transition  0\n",
      "2019-02-23 , transition  2\n",
      "2019-02-23 , transition  4\n",
      "2019-02-23 , transition  6\n",
      "2019-02-23 , transition  8\n",
      "2019-02-24 , transition  0\n",
      "2019-02-24 , transition  2\n",
      "2019-02-24 , transition  4\n",
      "2019-02-24 , transition  6\n",
      "2019-02-24 , transition  8\n",
      "2019-02-25 , transition  0\n",
      "2019-02-25 , transition  2\n",
      "2019-02-25 , transition  4\n",
      "2019-02-25 , transition  6\n",
      "2019-02-25 , transition  8\n",
      "2019-02-26 , transition  0\n",
      "2019-02-26 , transition  2\n",
      "2019-02-26 , transition  4\n",
      "2019-02-26 , transition  6\n",
      "2019-02-26 , transition  8\n",
      "2019-02-27 , transition  0\n",
      "2019-02-27 , transition  2\n",
      "2019-02-27 , transition  4\n",
      "2019-02-27 , transition  6\n",
      "2019-02-27 , transition  8\n",
      "2019-02-28 , transition  0\n",
      "2019-02-28 , transition  2\n",
      "2019-02-28 , transition  4\n",
      "2019-02-28 , transition  6\n",
      "2019-02-28 , transition  8\n",
      "2019-03-01 , transition  0\n",
      "2019-03-01 , transition  2\n",
      "2019-03-01 , transition  4\n",
      "2019-03-01 , transition  6\n",
      "2019-03-01 , transition  8\n",
      "2019-03-02 , transition  0\n",
      "2019-03-02 , transition  2\n",
      "2019-03-02 , transition  4\n",
      "2019-03-02 , transition  6\n",
      "2019-03-02 , transition  8\n",
      "2019-03-03 , transition  0\n",
      "2019-03-03 , transition  2\n",
      "2019-03-03 , transition  4\n",
      "2019-03-03 , transition  6\n",
      "2019-03-03 , transition  8\n",
      "2019-03-04 , transition  0\n",
      "2019-03-04 , transition  2\n",
      "2019-03-04 , transition  4\n",
      "2019-03-04 , transition  6\n",
      "2019-03-04 , transition  8\n",
      "2019-03-05 , transition  0\n",
      "2019-03-05 , transition  2\n",
      "2019-03-05 , transition  4\n",
      "2019-03-05 , transition  6\n",
      "2019-03-05 , transition  8\n",
      "2019-03-06 , transition  0\n",
      "2019-03-06 , transition  2\n",
      "2019-03-06 , transition  4\n",
      "2019-03-06 , transition  6\n",
      "2019-03-06 , transition  8\n",
      "2019-03-07 , transition  0\n",
      "2019-03-07 , transition  2\n",
      "2019-03-07 , transition  4\n",
      "2019-03-07 , transition  6\n",
      "2019-03-07 , transition  8\n",
      "2019-03-08 , transition  0\n",
      "2019-03-08 , transition  2\n",
      "2019-03-08 , transition  4\n",
      "2019-03-08 , transition  6\n",
      "2019-03-08 , transition  8\n",
      "2019-03-09 , transition  0\n",
      "2019-03-09 , transition  2\n",
      "2019-03-09 , transition  4\n",
      "2019-03-09 , transition  6\n",
      "2019-03-09 , transition  8\n",
      "2019-03-10 , transition  0\n",
      "2019-03-10 , transition  2\n",
      "2019-03-10 , transition  4\n",
      "2019-03-10 , transition  6\n",
      "2019-03-10 , transition  8\n",
      "2019-03-11 , transition  0\n",
      "2019-03-11 , transition  2\n",
      "2019-03-11 , transition  4\n",
      "2019-03-11 , transition  6\n",
      "2019-03-11 , transition  8\n",
      "2019-03-12 , transition  0\n",
      "2019-03-12 , transition  2\n",
      "2019-03-12 , transition  4\n",
      "2019-03-12 , transition  6\n",
      "2019-03-12 , transition  8\n",
      "2019-03-13 , transition  0\n",
      "2019-03-13 , transition  2\n",
      "2019-03-13 , transition  4\n",
      "2019-03-13 , transition  6\n",
      "2019-03-13 , transition  8\n",
      "2019-03-14 , transition  0\n",
      "2019-03-14 , transition  2\n",
      "2019-03-14 , transition  4\n",
      "2019-03-14 , transition  6\n",
      "2019-03-14 , transition  8\n",
      "2019-03-15 , transition  0\n",
      "2019-03-15 , transition  2\n",
      "2019-03-15 , transition  4\n",
      "2019-03-15 , transition  6\n",
      "2019-03-15 , transition  8\n",
      "2019-03-16 , transition  0\n",
      "2019-03-16 , transition  2\n",
      "2019-03-16 , transition  4\n",
      "2019-03-16 , transition  6\n",
      "2019-03-16 , transition  8\n",
      "2019-03-17 , transition  0\n",
      "2019-03-17 , transition  2\n",
      "2019-03-17 , transition  4\n",
      "2019-03-17 , transition  6\n",
      "2019-03-17 , transition  8\n",
      "2019-03-18 , transition  0\n",
      "2019-03-18 , transition  2\n",
      "2019-03-18 , transition  4\n",
      "2019-03-18 , transition  6\n",
      "2019-03-18 , transition  8\n",
      "2019-03-19 , transition  0\n",
      "2019-03-19 , transition  2\n",
      "2019-03-19 , transition  4\n",
      "2019-03-19 , transition  6\n",
      "2019-03-19 , transition  8\n",
      "2019-03-20 , transition  0\n",
      "2019-03-20 , transition  2\n",
      "2019-03-20 , transition  4\n",
      "2019-03-20 , transition  6\n",
      "2019-03-20 , transition  8\n",
      "2019-03-21 , transition  0\n",
      "2019-03-21 , transition  2\n",
      "2019-03-21 , transition  4\n",
      "2019-03-21 , transition  6\n",
      "2019-03-21 , transition  8\n",
      "2019-03-22 , transition  0\n",
      "2019-03-22 , transition  2\n",
      "2019-03-22 , transition  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-22 , transition  6\n",
      "2019-03-22 , transition  8\n",
      "2019-03-23 , transition  0\n",
      "2019-03-23 , transition  2\n",
      "2019-03-23 , transition  4\n",
      "2019-03-23 , transition  6\n",
      "2019-03-23 , transition  8\n",
      "2019-03-24 , transition  0\n",
      "2019-03-24 , transition  2\n",
      "2019-03-24 , transition  4\n",
      "2019-03-24 , transition  6\n",
      "2019-03-24 , transition  8\n",
      "2019-03-25 , transition  0\n",
      "2019-03-25 , transition  2\n",
      "2019-03-25 , transition  4\n",
      "2019-03-25 , transition  6\n",
      "2019-03-25 , transition  8\n",
      "2019-03-26 , transition  0\n",
      "2019-03-26 , transition  2\n",
      "2019-03-26 , transition  4\n",
      "2019-03-26 , transition  6\n",
      "2019-03-26 , transition  8\n",
      "2019-03-27 , transition  0\n",
      "2019-03-27 , transition  2\n",
      "2019-03-27 , transition  4\n",
      "2019-03-27 , transition  6\n",
      "2019-03-27 , transition  8\n",
      "2019-03-28 , transition  0\n",
      "2019-03-28 , transition  2\n",
      "2019-03-28 , transition  4\n",
      "2019-03-28 , transition  6\n",
      "2019-03-28 , transition  8\n",
      "2019-03-29 , transition  0\n",
      "2019-03-29 , transition  2\n",
      "2019-03-29 , transition  4\n",
      "2019-03-29 , transition  6\n",
      "2019-03-29 , transition  8\n",
      "2019-03-30 , transition  0\n",
      "2019-03-30 , transition  2\n",
      "2019-03-30 , transition  4\n",
      "2019-03-30 , transition  6\n",
      "2019-03-30 , transition  8\n",
      "2019-03-31 , transition  0\n",
      "2019-03-31 , transition  2\n",
      "2019-03-31 , transition  4\n",
      "2019-03-31 , transition  6\n",
      "2019-03-31 , transition  8\n",
      "2019-04-01 , transition  0\n",
      "2019-04-01 , transition  2\n",
      "2019-04-01 , transition  4\n",
      "2019-04-01 , transition  6\n",
      "2019-04-01 , transition  8\n",
      "2019-04-02 , transition  0\n",
      "2019-04-02 , transition  2\n",
      "2019-04-02 , transition  4\n",
      "2019-04-02 , transition  6\n",
      "2019-04-02 , transition  8\n",
      "2019-04-03 , transition  0\n",
      "2019-04-03 , transition  2\n",
      "2019-04-03 , transition  4\n",
      "2019-04-03 , transition  6\n",
      "2019-04-03 , transition  8\n",
      "2019-04-04 , transition  0\n",
      "2019-04-04 , transition  2\n",
      "2019-04-04 , transition  4\n",
      "2019-04-04 , transition  6\n",
      "2019-04-04 , transition  8\n",
      "2019-04-05 , transition  0\n",
      "2019-04-05 , transition  2\n",
      "2019-04-05 , transition  4\n",
      "2019-04-05 , transition  6\n",
      "2019-04-05 , transition  8\n",
      "2019-04-06 , transition  0\n",
      "2019-04-06 , transition  2\n",
      "2019-04-06 , transition  4\n",
      "2019-04-06 , transition  6\n",
      "2019-04-06 , transition  8\n",
      "2019-04-07 , transition  0\n",
      "2019-04-07 , transition  2\n",
      "2019-04-07 , transition  4\n",
      "2019-04-07 , transition  6\n",
      "2019-04-07 , transition  8\n",
      "2019-04-08 , transition  0\n",
      "2019-04-08 , transition  2\n",
      "2019-04-08 , transition  4\n",
      "2019-04-08 , transition  6\n",
      "2019-04-08 , transition  8\n",
      "2019-04-09 , transition  0\n",
      "2019-04-09 , transition  2\n",
      "2019-04-09 , transition  4\n",
      "2019-04-09 , transition  6\n",
      "2019-04-09 , transition  8\n",
      "2019-04-10 , transition  0\n",
      "2019-04-10 , transition  2\n",
      "2019-04-10 , transition  4\n",
      "2019-04-10 , transition  6\n",
      "2019-04-10 , transition  8\n",
      "2019-04-11 , transition  0\n",
      "2019-04-11 , transition  2\n",
      "2019-04-11 , transition  4\n",
      "2019-04-11 , transition  6\n",
      "2019-04-11 , transition  8\n",
      "2019-04-12 , transition  0\n",
      "2019-04-12 , transition  2\n",
      "2019-04-12 , transition  4\n",
      "2019-04-12 , transition  6\n",
      "2019-04-12 , transition  8\n",
      "2019-04-13 , transition  0\n",
      "2019-04-13 , transition  2\n",
      "2019-04-13 , transition  4\n",
      "2019-04-13 , transition  6\n",
      "2019-04-13 , transition  8\n",
      "2019-04-14 , transition  0\n",
      "2019-04-14 , transition  2\n",
      "2019-04-14 , transition  4\n",
      "2019-04-14 , transition  6\n",
      "2019-04-14 , transition  8\n",
      "2019-04-15 , transition  0\n",
      "2019-04-15 , transition  2\n",
      "2019-04-15 , transition  4\n",
      "2019-04-15 , transition  6\n",
      "2019-04-15 , transition  8\n",
      "2019-04-16 , transition  0\n",
      "2019-04-16 , transition  2\n",
      "2019-04-16 , transition  4\n",
      "2019-04-16 , transition  6\n",
      "2019-04-16 , transition  8\n",
      "2019-04-17 , transition  0\n",
      "2019-04-17 , transition  2\n",
      "2019-04-17 , transition  4\n",
      "2019-04-17 , transition  6\n",
      "2019-04-17 , transition  8\n",
      "2019-04-18 , transition  0\n",
      "2019-04-18 , transition  2\n",
      "2019-04-18 , transition  4\n",
      "2019-04-18 , transition  6\n",
      "2019-04-18 , transition  8\n",
      "2019-04-19 , transition  0\n",
      "2019-04-19 , transition  2\n",
      "2019-04-19 , transition  4\n",
      "2019-04-19 , transition  6\n",
      "2019-04-19 , transition  8\n",
      "2019-04-20 , transition  0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageFont\n",
    "from PIL import ImageDraw \n",
    "\n",
    "x_scale = 1.0 * img_pix_x / pack_img_pix_x\n",
    "y_scale = 1.0 * img_pix_y / pack_img_pix_y\n",
    "\n",
    "# group by days\n",
    "grouped = dfChart.groupby(['date', 'transition'])\n",
    "for date, group in grouped:\n",
    "    date_str = date[0]\n",
    "    transition = date[1]\n",
    "    \n",
    "    # skip odd ones, too jerky otherwize\n",
    "    if transition%2 != 0:\n",
    "        continue\n",
    "    \n",
    "    print (date_str, \", transition \", transition)\n",
    "    \n",
    "    # generate combined image of one day\n",
    "    first = True\n",
    "    for row_index, row in group.iterrows():\n",
    "        radius = x_scale * row.radius\n",
    "        if radius > 0:\n",
    "            # creating background\n",
    "            if first:\n",
    "                background = Image.new('RGB', (img_pix_x,img_pix_y), (255,255,255))\n",
    "                first = False\n",
    "                continue\n",
    "\n",
    "            # resize current image to currend radius\n",
    "            img = images[row.cluster]\n",
    "            r = math.ceil(radius)\n",
    "            img = img.resize((r, r), resample=Image.ANTIALIAS)\n",
    "\n",
    "            # place\n",
    "            pos = (math.ceil(x_scale*row.x - r/2), math.ceil(y_scale*row.y - r/2))\n",
    "\n",
    "            # combine images\n",
    "            background.paste(img, pos, img)\n",
    "        \n",
    "    # print date\n",
    "    draw = ImageDraw.Draw(background)\n",
    "    font = ImageFont.truetype(\"arial.ttf\", 72)\n",
    "    draw.text((math.ceil(x_scale*50), math.ceil(x_scale*50)),date_str,(55,55,55), font=font)\n",
    "\n",
    "    # save one day combine image\n",
    "    background.save(\"wc5/wc.{0}.{1}.jpg\".format(date_str,transition), \"JPEG\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
