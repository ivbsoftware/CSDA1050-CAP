# GDELT3
This folder contains code used to generate the presentation.

## Steps to reproduce
Python code used in this section located in [gta-news](https://github.com/ivbsoftware/CSDA1050-CAP/tree/master/playground/GDELT3/gta-news/doc2vec) folder.

### Step 1
Download, clean and tokenize the Haffington Post news corpus. Parameters of this script are start line number, end line number (-1 if to the end) and suffix of the file generated. This is in case multiple processes are needed to run to speed up the tokenizing.

```python
python prepare_train_corpus.py 0 -1 0 
```
### Step 2
Train Doc2Vec model.

```python
python train_doc2vec.py
```
### Step 3
Test your model to check if the results make sense. Code below will run several tests.

```python
python inspect_doc2vec.py
```
### Step 4
Extract/update GDELT links related to GTA area. The script runs 60 days back, skipping previosly downloaded and processed files. Can be run daily to update with lates data.

```python
python get_gta_gdelt_links.py
```

### Step 5
Process files created on the Step 4. Daily files are loaded, for each url the body of the article is extracted and tokenized. Each day file is saved to __data\gta-gdelt-corpus__ folder.

```python
python prepare_train_corpus.py
```
## Data generated by the code
The data files generated here can be downloaded from [Google Drive](https://drive.google.com/open?id=1sXD0DDlBfDKu0AnKXqoxSb92WftFqJEo). 
It contains the following artifacts:

 - __News_Category_Dataset_v2.json__ is a list of 200K links of news articles from Haffington Post site taken from [Kaggle News Category Dataset](https://www.kaggle.com/rmisra/news-category-dataset).
 - __news-dataset.zip__ is a dataset containing cleaned tokenized articles from the source above.
 - __news-dataset-huffington.model__ is a Doc2Vec model trained on 186,513 articles (those having more than 10 tokens).
