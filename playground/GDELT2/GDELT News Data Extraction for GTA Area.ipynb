{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GDELT News Data Extraction for GTA Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20190408.export.CSV.zip',\n",
       " '20190407.export.CSV.zip',\n",
       " '20190406.export.CSV.zip',\n",
       " '20190405.export.CSV.zip',\n",
       " '20190404.export.CSV.zip']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import lxml.html as lh\n",
    "\n",
    "gdelt_base_url = 'http://data.gdeltproject.org/events/'\n",
    "\n",
    "# get the list of all the links on the gdelt file page\n",
    "page = requests.get(gdelt_base_url+'index.html')\n",
    "doc = lh.fromstring(page.content)\n",
    "link_list = doc.xpath(\"//*/ul/li/a/@href\")\n",
    "\n",
    "# separate out those links that begin with four digits \n",
    "file_list = [x for x in link_list if str.isdigit(x[0:4])]\n",
    "\n",
    "# preview the list\n",
    "file_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geo-fence\n",
    "lt1 = 43.403221\n",
    "lt2 = 43.855401\n",
    "lg1 = -79.639319\n",
    "lg2 = -78.905820\n",
    "\n",
    "# days back to process\n",
    "days_back = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working dir: C:\\Users\\ibaranov\\Downloads\\York\\group-projects\\CSDA1050-CAP\\playground\\GDELT2\n",
      "data\\20190408.export.CSV.zip\n",
      "...extracting,\n",
      "...parsing,\n",
      "data\\20190407.export.CSV.zip\n",
      "...downloading,\n",
      "...extracting,\n",
      "...parsing,\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import urllib\n",
    "import zipfile\n",
    "import glob\n",
    "import operator\n",
    "\n",
    "infilecounter = 0\n",
    "outfilecounter = 0\n",
    "\n",
    "dir_path = os.getcwd()\n",
    "print ('Working dir: ' + dir_path)\n",
    "                          \n",
    "# make some dirs\n",
    "local_path = dir_path + '\\\\'\n",
    "if not os.path.exists(local_path+'data'):\n",
    "    os.makedirs(local_path+'data')\n",
    "if not os.path.exists(local_path+'gta-data'):\n",
    "    os.makedirs(local_path+'gta-data')\n",
    "    \n",
    "for compressed_file in file_list[infilecounter:]:\n",
    "    print ('data\\\\'+compressed_file),\n",
    "    \n",
    "    # if we dont have the compressed file stored locally, go get it. Keep trying if necessary.\n",
    "    while not os.path.isfile(local_path+'data\\\\'+compressed_file): \n",
    "        print ('...downloading,'),\n",
    "        urllib.request.urlretrieve(url=gdelt_base_url+compressed_file, \n",
    "                                   filename=local_path+'data\\\\'+compressed_file)\n",
    "        \n",
    "    # extract the contents of the compressed file to a temporary directory    \n",
    "    print ('...extracting,'),\n",
    "    z = zipfile.ZipFile(file=local_path + 'data\\\\' + compressed_file, mode='r')    \n",
    "    z.extractall(path=local_path + 'tmp/')\n",
    "    \n",
    "    # parse each of the csv files in the working directory, \n",
    "    print ('...parsing,'),\n",
    "    for infile_name in glob.glob(local_path + 'tmp/*'):\n",
    "        outfile_name = local_path + 'gta-data\\\\' + 'gta' + '%04i.tsv'%outfilecounter    \n",
    "    \n",
    "        # open the infile and outfile\n",
    "        with open(infile_name,  mode='r', encoding=\"utf8\") as infile, \\\n",
    "             open(outfile_name, mode='w', encoding=\"utf8\") as outfile:\n",
    "            \n",
    "            for line in infile:\n",
    "                vals = line.split('\\t')\n",
    "                \n",
    "                # extract geo-coordinates\n",
    "                try:\n",
    "                    lat  = float(vals[53]) # ActionGeo_Lat\n",
    "                    long = float(vals[54]) # ActionGeo_Long\n",
    "                except Exception as e:\n",
    "                    # means no coordinates provided, skipping\n",
    "                    continue\n",
    "                \n",
    "                # only use events inside geo-fence\n",
    "                if  long >= lg1 and long <= lg2 and lat >= lt1 and lat <= lt2:\n",
    "                    outfile.write(line)\n",
    "                \n",
    "            outfilecounter +=1\n",
    "            \n",
    "        # delete the temporary file\n",
    "        os.remove(infile_name)\n",
    "        \n",
    "    infilecounter +=1\n",
    "    if infilecounter >= days_back:\n",
    "        print ('done')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ibaranov\\Downloads\\York\\group-projects\\CSDA1050-CAP\\playground\\GDELT2\\gta-data\\gta0000.tsv\n",
      "C:\\Users\\ibaranov\\Downloads\\York\\group-projects\\CSDA1050-CAP\\playground\\GDELT2\\gta-data\\gta0001.tsv\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Get the GDELT field names from an external helper file\n",
    "colnames = pd.read_excel('CSV.header.fieldids.xlsx', sheet_name='Sheet1', \n",
    "                         index_col='Column ID', usecols=1)['Field Name']\n",
    "\n",
    "# Build DataFrames from each of the intermediary files\n",
    "files = glob.glob(local_path+'gta-data/'+'*')\n",
    "DFlist = []\n",
    "for active_file in files:\n",
    "    print (active_file)\n",
    "    DFlist.append(pd.read_csv(active_file, sep='\\t', header=None, dtype=str,\n",
    "                              names=colnames, index_col=['GLOBALEVENTID']))\n",
    "\n",
    "# Merge the file-based dataframes and serialize the dataframe\n",
    "DF = pd.concat(DFlist)\n",
    "DF.to_pickle(local_path+'backup'+'-gta'+'.pickle')    \n",
    "    \n",
    "# remove the temporary files\n",
    "for active_file in files:\n",
    "    os.remove(active_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SQLDATE',\n",
       " 'MonthYear',\n",
       " 'Year',\n",
       " 'FractionDate',\n",
       " 'Actor1Code',\n",
       " 'Actor1Name',\n",
       " 'Actor1CountryCode',\n",
       " 'Actor1KnownGroupCode',\n",
       " 'Actor1EthnicCode',\n",
       " 'Actor1Religion1Code',\n",
       " 'Actor1Religion2Code',\n",
       " 'Actor1Type1Code',\n",
       " 'Actor1Type2Code',\n",
       " 'Actor1Type3Code',\n",
       " 'Actor2Code',\n",
       " 'Actor2Name',\n",
       " 'Actor2CountryCode',\n",
       " 'Actor2KnownGroupCode',\n",
       " 'Actor2EthnicCode',\n",
       " 'Actor2Religion1Code',\n",
       " 'Actor2Religion2Code',\n",
       " 'Actor2Type1Code',\n",
       " 'Actor2Type2Code',\n",
       " 'Actor2Type3Code',\n",
       " 'IsRootEvent',\n",
       " 'EventCode',\n",
       " 'EventBaseCode',\n",
       " 'EventRootCode',\n",
       " 'QuadClass',\n",
       " 'GoldsteinScale',\n",
       " 'NumMentions',\n",
       " 'NumSources',\n",
       " 'NumArticles',\n",
       " 'AvgTone',\n",
       " 'Actor1Geo_Type',\n",
       " 'Actor1Geo_FullName',\n",
       " 'Actor1Geo_CountryCode',\n",
       " 'Actor1Geo_ADM1Code',\n",
       " 'Actor1Geo_Lat',\n",
       " 'Actor1Geo_Long',\n",
       " 'Actor1Geo_FeatureID',\n",
       " 'Actor2Geo_Type',\n",
       " 'Actor2Geo_FullName',\n",
       " 'Actor2Geo_CountryCode',\n",
       " 'Actor2Geo_ADM1Code',\n",
       " 'Actor2Geo_Lat',\n",
       " 'Actor2Geo_Long',\n",
       " 'Actor2Geo_FeatureID',\n",
       " 'ActionGeo_Type',\n",
       " 'ActionGeo_FullName',\n",
       " 'ActionGeo_CountryCode',\n",
       " 'ActionGeo_ADM1Code',\n",
       " 'ActionGeo_Lat',\n",
       " 'ActionGeo_Long',\n",
       " 'ActionGeo_FeatureID',\n",
       " 'DATEADDED',\n",
       " 'SOURCEURL']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(DF)\n",
    "#print(DF.iloc[0][-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
